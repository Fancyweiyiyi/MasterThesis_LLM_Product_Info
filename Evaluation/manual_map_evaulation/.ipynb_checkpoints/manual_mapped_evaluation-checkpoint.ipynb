{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "364b1377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 Metrics: Precision: 0.19, Recall: 0.93, F1-Score: 0.31, Coverage: 13.85\n",
      "GPT-4o Mini Metrics: Precision: 0.38, Recall: 0.32, F1-Score: 0.35, Coverage: 0.77\n",
      "LLaMA 3.1 Metrics: Precision: 0.20, Recall: 0.90, F1-Score: 0.33, Coverage: 10.80\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the ground truth and mapping table\n",
    "ground_truth_path = \"ground_truth.xlsx\"\n",
    "mapping_table_path = \"Manual_Mapping_Table.xlsx\"\n",
    "ground_truth = pd.read_excel(ground_truth_path)\n",
    "manual_mapping_table = pd.read_excel(mapping_table_path)\n",
    "\n",
    "# Load the model outputs\n",
    "chatgpt4_output = pd.read_csv(\"raw_results_chatgpt4_statistic.csv\")\n",
    "chatgpt4omini_output = pd.read_csv(\"raw_results_chatgpt4omini.csv\")\n",
    "llama3_output = pd.read_csv(\"raw_results_llama3_1.csv\")\n",
    "\n",
    "# Normalize data for consistency\n",
    "manual_mapping_table[\"Extracted Attribute\"] = manual_mapping_table[\"Extracted Attribute\"].str.lower().str.strip()\n",
    "manual_mapping_table[\"Mapped Attribute\"] = manual_mapping_table[\"Mapped Attribute\"].str.lower().str.strip()\n",
    "manual_mapping_table[\"Category\"] = manual_mapping_table[\"Category\"].str.lower().str.strip()\n",
    "\n",
    "ground_truth[\"Attribute\"] = ground_truth[\"Attribute\"].str.lower().str.strip()\n",
    "ground_truth[\"Category\"] = ground_truth[\"Category\"].str.lower().str.strip()\n",
    "\n",
    "for output in [chatgpt4_output, chatgpt4omini_output, llama3_output]:\n",
    "    output[\"Attribute\"] = output[\"Attribute\"].str.lower().str.strip()\n",
    "    output[\"Category\"] = output[\"Category\"].str.lower().str.strip()\n",
    "\n",
    "# Apply manual mapping to model outputs\n",
    "def apply_mapping(model_output, mapping_table):\n",
    "    \"\"\"\n",
    "    Map extracted attributes in model output to ground truth attributes using the consolidated mapping table.\n",
    "    \"\"\"\n",
    "    # Merge model output with the mapping table\n",
    "    mapped_output = model_output.merge(\n",
    "        mapping_table,\n",
    "        left_on=[\"Category\", \"Attribute\"],\n",
    "        right_on=[\"Category\", \"Extracted Attribute\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Use the 'Mapped Attribute' if matched, or retain the original attribute if not\n",
    "    mapped_output[\"Mapped Attribute\"] = mapped_output[\"Mapped Attribute\"].fillna(mapped_output[\"Attribute\"])\n",
    "    \n",
    "    # Keep only relevant columns\n",
    "    mapped_output = mapped_output[[\"Category\", \"Mapped Attribute\", \"Count\", \"Percentage\"]]\n",
    "\n",
    "    return mapped_output\n",
    "\n",
    "# Apply the mapping function to each model output\n",
    "mapped_chatgpt4 = apply_mapping(chatgpt4_output, manual_mapping_table)\n",
    "mapped_chatgpt4omini = apply_mapping(chatgpt4omini_output, manual_mapping_table)\n",
    "mapped_llama3 = apply_mapping(llama3_output, manual_mapping_table)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "def calculate_metrics(mapped_output, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate Precision, Recall, F1-Score, and Coverage based on mapped attributes.\n",
    "    \"\"\"\n",
    "    # True positives\n",
    "    true_positives = mapped_output[mapped_output[\"Mapped Attribute\"].isin(ground_truth[\"Attribute\"])]\n",
    "    \n",
    "    # False positives\n",
    "    false_positives = mapped_output[~mapped_output[\"Mapped Attribute\"].isin(ground_truth[\"Attribute\"])]\n",
    "    \n",
    "    # False negatives\n",
    "    false_negatives = ground_truth[~ground_truth[\"Attribute\"].isin(mapped_output[\"Mapped Attribute\"])]\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = len(true_positives) / (len(true_positives) + len(false_positives)) if len(true_positives) + len(false_positives) > 0 else 0\n",
    "    recall = len(true_positives) / (len(true_positives) + len(false_negatives)) if len(true_positives) + len(false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    coverage = mapped_output[\"Mapped Attribute\"].nunique() / ground_truth[\"Attribute\"].nunique()\n",
    "\n",
    "    return precision, recall, f1_score, coverage\n",
    "\n",
    "# Evaluate metrics for each model\n",
    "metrics_chatgpt4 = calculate_metrics(mapped_chatgpt4, ground_truth)\n",
    "metrics_chatgpt4omini = calculate_metrics(mapped_chatgpt4omini, ground_truth)\n",
    "metrics_llama3 = calculate_metrics(mapped_llama3, ground_truth)\n",
    "\n",
    "# Print results\n",
    "print(f\"GPT-4 Metrics: Precision: {metrics_chatgpt4[0]:.2f}, Recall: {metrics_chatgpt4[1]:.2f}, F1-Score: {metrics_chatgpt4[2]:.2f}, Coverage: {metrics_chatgpt4[3]:.2f}\")\n",
    "print(f\"GPT-4o Mini Metrics: Precision: {metrics_chatgpt4omini[0]:.2f}, Recall: {metrics_chatgpt4omini[1]:.2f}, F1-Score: {metrics_chatgpt4omini[2]:.2f}, Coverage: {metrics_chatgpt4omini[3]:.2f}\")\n",
    "print(f\"LLaMA 3.1 Metrics: Precision: {metrics_llama3[0]:.2f}, Recall: {metrics_llama3[1]:.2f}, F1-Score: {metrics_llama3[2]:.2f}, Coverage: {metrics_llama3[3]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c5c6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_chatgpt4.to_csv('mapped_chatgpt4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a45c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_chatgpt4omini.to_csv('mapped_chatgpt4omini.csv')\n",
    "mapped_llama3.to_csv('mapped_llama3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
