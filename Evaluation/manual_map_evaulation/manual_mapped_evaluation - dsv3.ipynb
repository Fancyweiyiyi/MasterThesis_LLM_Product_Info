{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "364b1377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsv3 Metrics: Precision: 0.16, Recall: 0.80, F1-Score: 0.27, Coverage: 8.21\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the ground truth and mapping table\n",
    "ground_truth_path = \"ground_truth.xlsx\"\n",
    "mapping_table_path = \"Manual_Mapping_Table.xlsx\"\n",
    "ground_truth = pd.read_excel(ground_truth_path)\n",
    "manual_mapping_table = pd.read_excel(mapping_table_path)\n",
    "\n",
    "# Load the model outputs\n",
    "dsv3_output = pd.read_csv(\"DeepSeekV3_raw_Results.csv\")\n",
    "\n",
    "\n",
    "# Normalize data for consistency\n",
    "manual_mapping_table[\"Extracted Attribute\"] = manual_mapping_table[\"Extracted Attribute\"].str.lower().str.strip()\n",
    "manual_mapping_table[\"Mapped Attribute\"] = manual_mapping_table[\"Mapped Attribute\"].str.lower().str.strip()\n",
    "manual_mapping_table[\"Category\"] = manual_mapping_table[\"Category\"].str.lower().str.strip()\n",
    "\n",
    "ground_truth[\"Attribute\"] = ground_truth[\"Attribute\"].str.lower().str.strip()\n",
    "ground_truth[\"Category\"] = ground_truth[\"Category\"].str.lower().str.strip()\n",
    "\n",
    "for output in [dsv3_output]:\n",
    "    output[\"Attribute\"] = output[\"Attribute\"].str.lower().str.strip()\n",
    "    output[\"Category\"] = output[\"Category\"].str.lower().str.strip()\n",
    "\n",
    "# Apply manual mapping to model outputs\n",
    "def apply_mapping(model_output, mapping_table):\n",
    "    \"\"\"\n",
    "    Map extracted attributes in model output to ground truth attributes using the consolidated mapping table.\n",
    "    \"\"\"\n",
    "    # Merge model output with the mapping table\n",
    "    mapped_output = model_output.merge(\n",
    "        mapping_table,\n",
    "        left_on=[\"Category\", \"Attribute\"],\n",
    "        right_on=[\"Category\", \"Extracted Attribute\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Use the 'Mapped Attribute' if matched, or retain the original attribute if not\n",
    "    mapped_output[\"Mapped Attribute\"] = mapped_output[\"Mapped Attribute\"].fillna(mapped_output[\"Attribute\"])\n",
    "    \n",
    "    # Keep only relevant columns\n",
    "    mapped_output = mapped_output[[\"Category\", \"Mapped Attribute\", \"Count\", \"Percentage\"]]\n",
    "\n",
    "    return mapped_output\n",
    "\n",
    "# Apply the mapping function to each model output\n",
    "mapped_dsv3 = apply_mapping(dsv3_output, manual_mapping_table)\n",
    "\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "def calculate_metrics(mapped_output, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate Precision, Recall, F1-Score, and Coverage based on mapped attributes.\n",
    "    \"\"\"\n",
    "    # True positives\n",
    "    true_positives = mapped_output[mapped_output[\"Mapped Attribute\"].isin(ground_truth[\"Attribute\"])]\n",
    "    \n",
    "    # False positives\n",
    "    false_positives = mapped_output[~mapped_output[\"Mapped Attribute\"].isin(ground_truth[\"Attribute\"])]\n",
    "    \n",
    "    # False negatives\n",
    "    false_negatives = ground_truth[~ground_truth[\"Attribute\"].isin(mapped_output[\"Mapped Attribute\"])]\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = len(true_positives) / (len(true_positives) + len(false_positives)) if len(true_positives) + len(false_positives) > 0 else 0\n",
    "    recall = len(true_positives) / (len(true_positives) + len(false_negatives)) if len(true_positives) + len(false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    coverage = mapped_output[\"Mapped Attribute\"].nunique() / ground_truth[\"Attribute\"].nunique()\n",
    "\n",
    "    return precision, recall, f1_score, coverage\n",
    "\n",
    "# Evaluate metrics for each model\n",
    "metrics_dsv3 = calculate_metrics(mapped_dsv3, ground_truth)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"dsv3 Metrics: Precision: {metrics_dsv3[0]:.2f}, Recall: {metrics_dsv3[1]:.2f}, F1-Score: {metrics_dsv3[2]:.2f}, Coverage: {metrics_dsv3[3]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5c6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_dsv3.to_csv('mapped_dsv3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a45c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
